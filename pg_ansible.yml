---
#==============================================================#
# File      :   pg_ansible.yml
# Desc      :   pg_ansible complete config example
# Ctime     :   2024-Jul-01
# Mtime     :   2024-Jul-03
# Docs      :   https://github.com/msavdert/pg_ansible
# Author    :   Melih Savdert (melihsavdert@gmail.com)
# License   :   AGPLv3
#==============================================================#

all:
  ##################################################################
  #                            CLUSTERS                            #
  ##################################################################
  # meta nodes, nodes, pgsql, pgsql clusters are defined as
  # k:v pair inside `all.children`. Where the key is cluster name
  # and value is cluster definition consist of two parts:
  # `hosts`: cluster members ip and instance level variables
  # `vars` : cluster level variables
  ##################################################################
  children: # groups definition
    #----------------------------------#
    # pgsql cluster: pg-test (1 nodes) #
    #----------------------------------#
    # pg-test --->  172.28.5.161
    pg-test: # define the new 1-node cluster pg-test
      hosts:
        172.28.5.161:
          {
            pg_seq: 1,
            pg_role: primary,
            ansible_user: root,
            ansible_ssh_pass: root,
          }
      vars:
        pg_cluster: pg-test # define pgsql cluster name
        pg_users:
          [
            {
              name: test,
              password: test,
              pgbouncer: true,
              roles: [dbrole_admin],
            },
          ]
        pg_databases: [{ name: test }] # create a database and user named 'test'
        node_tune: tiny
        pg_conf: tiny.yml
        node_crontab: # make a full backup on monday 1am, and an incremental backup during weekdays
          - "00 01 * * 1 postgres /pg/bin/pg-backup full"
          - "00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup"

    #----------------------------------#
    # pgsql cluster: pg-oci (1 nodes) #
    #----------------------------------#
    # pg-oci --->  172.28.5.162
    pg-oci: # define the new 1-node cluster pg-oci
      hosts:
        172.28.5.162:
          {
            pg_seq: 1,
            pg_role: primary,
            ansible_user: root,
            ansible_ssh_pass: root,
          }
      vars:
        pg_cluster: pg-oci # define pgsql cluster name
        pg_users:
          [
            {
              name: usr_testoci,
              password: usr_testoci,
              pgbouncer: true,
              roles: [dbrole_admin],
            },
          ]
        pg_databases: [{ name: db_testoci }] # create a database and user named 'test'
        node_tune: tiny
        pg_conf: tiny.yml
        node_crontab: # make a full backup on monday 1am, and an incremental backup during weekdays
          - "00 01 * * 1 postgres /pg/bin/pg-backup full"
          - "00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup"

    #----------------------------------#
    # pgsql cluster: pg-melih (1 nodes) #
    #----------------------------------#
    # pg-melih --->  172.28.5.163
    pg-melih: # define the new 1-node cluster pg-melih
      hosts:
        172.28.5.163:
          {
            pg_seq: 1,
            pg_role: primary,
            ansible_user: root,
            ansible_ssh_pass: root,
          }
      vars:
        pg_cluster: pg-melih # define pgsql cluster name
        pg_users:
          [
            {
              name: usr_testmelih,
              password: usr_testmelih,
              pgbouncer: true,
              roles: [dbrole_admin],
            },
          ]
        pg_databases: [{ name: db_testoci }] # create a database and user named 'test'
        node_tune: tiny
        pg_conf: tiny.yml
        node_crontab: # make a full backup on monday 1am, and an incremental backup during weekdays
          - "00 01 * * 1 postgres /pg/bin/pg-backup full"
          - "00 01 * * 2,3,4,5,6,7 postgres /pg/bin/pg-backup"

  ####################################################################
  #                             VARS                                 #
  ####################################################################
  vars: # global variables
    #================================================================#
    #                         VARS: NODE                             #
    #================================================================#

    #-----------------------------------------------------------------
    # NODE_IDENTITY
    #-----------------------------------------------------------------
    #nodename:           # [INSTANCE] # node instance identity, use hostname if missing, optional
    node_cluster: nodes # [CLUSTER] # node cluster identity, use 'nodes' if missing, optional
    nodename_overwrite: true # overwrite node's hostname with nodename?
    nodename_exchange: false # exchange nodename among play hosts?
    node_id_from_pg: true # use postgres identity as node identity if applicable?

    #-----------------------------------------------------------------
    # NODE_PACKAGE
    #-----------------------------------------------------------------
    node_repo_modules: local # upstream repo to be added on node, local by default
    node_repo_remove: true # remove existing repo on node?
    node_packages: [] # packages to be installed current nodes
    node_default_packages: # default packages to be installed on all nodes
      - lz4,unzip,bzip2,zlib,yum,pv,jq,git,ncdu,make,patch,bash,lsof,wget,uuid,tuned,nvme-cli,numactl,grubby,sysstat,iotop,htop,rsync,tcpdump,chrony,python3
      - netcat,socat,ftp,lrzsz,net-tools,ipvsadm,bind-utils,telnet,audit,ca-certificates,openssl,readline,vim-minimal,node_exporter,etcd,haproxy,python3-pip

    #-----------------------------------------------------------------
    # NODE_TUNE
    #-----------------------------------------------------------------
    node_disable_firewall: true # disable node firewall? true by default
    node_disable_selinux: true # disable node selinux? true by default
    node_disable_numa: false # disable node numa, reboot required
    node_disable_swap: false # disable node swap, use with caution
    node_static_network: true # preserve dns resolver settings after reboot
    node_disk_prefetch: false # setup disk prefetch on HDD to increase performance
    node_kernel_modules:
      [softdog, br_netfilter, ip_vs, ip_vs_rr, ip_vs_wrr, ip_vs_sh]
    node_hugepage_count: 0 # number of 2MB hugepage, take precedence over ratio
    node_hugepage_ratio: 0 # node mem hugepage ratio, 0 disable it by default
    node_overcommit_ratio: 0 # node mem overcommit ratio, 0 disable it by default
    node_tune: oltp # node tuned profile: none,oltp,olap,crit,tiny
    node_sysctl_params: {} # sysctl parameters in k:v format in addition to tuned

    #-----------------------------------------------------------------
    # NODE_ADMIN
    #-----------------------------------------------------------------
    node_data: /data # node main data directory, `/data` by default
    node_admin_enabled: true # create a admin user on target node?
    node_admin_uid: 88 # uid and gid for node admin user
    node_admin_username: dba # name of node admin user, `dba` by default
    node_admin_ssh_exchange: true # exchange admin ssh key among node cluster
    node_admin_pk_current: true # add current user's ssh pk to admin authorized_keys
    node_admin_pk_list: [] # ssh public keys to be added to admin user

    #-----------------------------------------------------------------
    # NODE_TIME
    #-----------------------------------------------------------------
    node_timezone: "America/New_York" # setup node timezone, empty string to skip
    node_ntp_enabled: true # enable chronyd time sync service?
    node_ntp_servers: # ntp servers in `/etc/chrony.conf`
      - pool pool.ntp.org iburst
    node_crontab_overwrite: true # overwrite or append to `/etc/crontab`?
    node_crontab: [] # crontab entries in `/etc/crontab`

    #-----------------------------------------------------------------
    # NODE_VIP
    #-----------------------------------------------------------------
    vip_enabled: false # enable vip on this node cluster?
    # vip_address:         [IDENTITY] # node vip address in ipv4 format, required if vip is enabled
    # vip_vrid:            [IDENTITY] # required, integer, 1-254, should be unique among same VLAN
    vip_role: backup # optional, `master/backup`, backup by default, use as init role
    vip_preempt: false # optional, `true/false`, false by default, enable vip preemption
    vip_interface: eth0 # node vip network interface to listen, `eth0` by default
    vip_dns_suffix: "" # node vip dns name suffix, empty string by default
    vip_exporter_port: 9650 # keepalived exporter listen port, 9650 by default

    #-----------------------------------------------------------------
    # HAPROXY
    #-----------------------------------------------------------------
    haproxy_enabled: true # enable haproxy on this node?
    haproxy_clean: false # cleanup all existing haproxy config?
    haproxy_reload: true # reload haproxy after config?
    haproxy_auth_enabled: true # enable authentication for haproxy admin page
    haproxy_admin_username: admin # haproxy admin username, `admin` by default
    haproxy_admin_password: pigsty # haproxy admin password, `pigsty` by default
    haproxy_exporter_port: 9101 # haproxy admin/exporter port, 9101 by default
    haproxy_client_timeout: 24h # client side connection timeout, 24h by default
    haproxy_server_timeout: 24h # server side connection timeout, 24h by default
    haproxy_services: [] # list of haproxy service to be exposed on node

    #================================================================#
    #                         VARS: ETCD                             #
    #================================================================#
    #etcd_seq: 1                      # etcd instance identifier, explicitly required
    #etcd_cluster: etcd               # etcd cluster & group name, etcd by default
    etcd_safeguard: false # prevent purging running etcd instance?
    etcd_clean: true # purging existing etcd during initialization?
    etcd_data: /data/etcd # etcd data directory, /data/etcd by default
    etcd_port: 2379 # etcd client port, 2379 by default
    etcd_peer_port: 2380 # etcd peer port, 2380 by default
    etcd_init: new # etcd initial cluster state, new or existing
    etcd_election_timeout: 1000 # etcd election timeout, 1000ms by default
    etcd_heartbeat_interval: 100 # etcd heartbeat interval, 100ms by default

    #================================================================#
    #                         VARS: MINIO                            #
    #================================================================#
    #minio_seq: 1                     # minio instance identifier, REQUIRED
    #minio_cluster: minio             # minio cluster name, minio by default
    minio_clean: false # cleanup minio during init?, false by default
    minio_user: minio # minio os user, `minio` by default
    minio_node: "${minio_cluster}-${minio_seq}.pigsty" # minio node name pattern
    minio_data: "/data/minio" # minio data dir(s), use {x...y} to specify multi drivers
    minio_domain: sss.pigsty # minio external domain name, `sss.pigsty` by default
    minio_port: 9000 # minio service port, 9000 by default
    minio_admin_port: 9001 # minio console port, 9001 by default
    minio_access_key: minioadmin # root access key, `minioadmin` by default
    minio_secret_key: minioadmin # root secret key, `minioadmin` by default
    minio_extra_vars: "" # extra environment variables
    minio_alias: sss # alias name for local minio deployment
    minio_buckets: [{ name: pgsql }, { name: infra }, { name: redis }]
    minio_users:
      - { access_key: dba, secret_key: S3User.DBA, policy: consoleAdmin }
      - { access_key: pgbackrest, secret_key: S3User.Backup, policy: readwrite }

    #================================================================#
    #                         VARS: PGSQL                            #
    #================================================================#

    #-----------------------------------------------------------------
    # PG_IDENTITY
    #-----------------------------------------------------------------
    pg_mode: pgsql #CLUSTER  # pgsql cluster mode: pgsql,citus,gpsql
    # pg_cluster:           #CLUSTER  # pgsql cluster name, required identity parameter
    # pg_seq: 0             #INSTANCE # pgsql instance seq number, required identity parameter
    # pg_role: replica      #INSTANCE # pgsql role, required, could be primary,replica,offline
    # pg_instances: {}      #INSTANCE # define multiple pg instances on node in `{port:ins_vars}` format
    # pg_upstream:          #INSTANCE # repl upstream ip addr for standby cluster or cascade replica
    # pg_shard:             #CLUSTER  # pgsql shard name, optional identity for sharding clusters
    # pg_group: 0           #CLUSTER  # pgsql shard index number, optional identity for sharding clusters
    # gp_role: master       #CLUSTER  # greenplum role of this cluster, could be master or segment
    pg_offline_query: false #INSTANCE # set to true to enable offline query on this instance

    #-----------------------------------------------------------------
    # PG_BUSINESS
    #-----------------------------------------------------------------
    # postgres business object definition, overwrite in group vars
    pg_users: [] # postgres business users
    pg_databases: [] # postgres business databases
    pg_services: [] # postgres business services
    pg_hba_rules: [] # business hba rules for postgres
    pgb_hba_rules: [] # business hba rules for pgbouncer
    # global credentials, overwrite in global vars
    pg_replication_username: replicator
    pg_replication_password: DBUser.Replicator
    pg_admin_username: dbuser_dba
    pg_admin_password: DBUser.DBA
    pg_monitor_username: dbuser_monitor
    pg_monitor_password: DBUser.Monitor
    pg_dbsu_password: "" # dbsu password, empty string means no dbsu password by default

    #-----------------------------------------------------------------
    # PG_INSTALL
    #-----------------------------------------------------------------
    pg_dbsu: postgres # os dbsu name, postgres by default, better not change it
    pg_dbsu_uid: 26 # os dbsu uid and gid, 26 for default postgres users and groups
    pg_dbsu_sudo: limit # dbsu sudo privilege, none,limit,all,nopass. limit by default
    pg_dbsu_home: /var/lib/pgsql # postgresql home directory, `/var/lib/pgsql` by default
    pg_dbsu_ssh_exchange: true # exchange postgres dbsu ssh key among same pgsql cluster
    pg_version: 16 # postgres major version to be installed, 16 by default
    pg_bin_dir: /usr/pgsql/bin # postgres binary dir, `/usr/pgsql/bin` by default
    pg_log_dir: /pg/log/postgres # postgres log dir, `/pg/log/postgres` by default
    pg_packages: # pg packages to be installed, `${pg_version}` will be replaced
      - postgresql${pg_version}*
      - pgbouncer pg_exporter pgbadger vip-manager patroni patroni-etcd pgbackrest
      - pg_repack_${pg_version}* wal2json_${pg_version}* passwordcheck_cracklib_${pg_version}* # important extensions
    pg_extensions: # pg extensions to be installed, `${pg_version}` will be replaced
      - postgis34_${pg_version}* timescaledb-2-postgresql-${pg_version}* pgvector_${pg_version}*

    #-----------------------------------------------------------------
    # PG_BOOTSTRAP
    #-----------------------------------------------------------------
    pg_safeguard: false # prevent purging running postgres instance? false by default
    pg_clean: true # purging existing postgres during pgsql init? true by default
    pg_data: /pg/data # postgres data directory, `/pg/data` by default
    pg_fs_main: /data # mountpoint/path for postgres main data, `/data` by default
    pg_fs_bkup: /data/backups # mountpoint/path for pg backup data, `/data/backup` by default
    pg_storage_type: SSD # storage type for pg main data, SSD,HDD, SSD by default
    pg_dummy_filesize: 64MiB # size of `/pg/dummy`, hold 64MB disk space for emergency use
    pg_listen: "0.0.0.0" # postgres/pgbouncer listen addresses, comma separated list
    pg_port: 5432 # postgres listen port, 5432 by default
    pg_localhost: /var/run/postgresql # postgres unix socket dir for localhost connection
    pg_namespace: /pg # top level key namespace in etcd, used by patroni & vip
    patroni_enabled: true # if disabled, no postgres cluster will be created during init
    patroni_mode: default # patroni working mode: default,pause,remove
    patroni_port: 8008 # patroni listen port, 8008 by default
    patroni_log_dir: /pg/log/patroni # patroni log dir, `/pg/log/patroni` by default
    patroni_ssl_enabled: false # secure patroni RestAPI communications with SSL?
    patroni_watchdog_mode: off # patroni watchdog mode: automatic,required,off. off by default
    patroni_username: postgres # patroni restapi username, `postgres` by default
    patroni_password: Patroni.API # patroni restapi password, `Patroni.API` by default
    patroni_citus_db: postgres # citus database managed by patroni, postgres by default
    pg_conf: oltp.yml # config template: oltp,olap,crit,tiny. `oltp.yml` by default
    pg_max_conn: auto # postgres max connections, `auto` will use recommended value
    pg_shared_buffer_ratio: 0.25 # postgres shared buffer ratio, 0.25 by default, 0.1~0.4
    pg_rto: 30 # recovery time objective in seconds,  `30s` by default
    pg_rpo: 1048576 # recovery point objective in bytes, `1MiB` at most by default
    pg_libs: "pg_stat_statements, auto_explain" # extensions to be loaded
    pg_delay: 0 # replication apply delay for standby cluster leader
    pg_checksum: false # enable data checksum for postgres cluster?
    pg_pwd_enc: scram-sha-256 # passwords encryption algorithm: md5,scram-sha-256
    pg_encoding: UTF8 # database cluster encoding, `UTF8` by default
    pg_locale: C # database cluster local, `C` by default
    pg_lc_collate: C # database cluster collate, `C` by default
    pg_lc_ctype: en_US.UTF8 # database character type, `en_US.UTF8` by default
    pgbouncer_enabled: true # if disabled, pgbouncer will not be launched on pgsql host
    pgbouncer_port: 6432 # pgbouncer listen port, 6432 by default
    pgbouncer_log_dir: /pg/log/pgbouncer # pgbouncer log dir, `/pg/log/pgbouncer` by default
    pgbouncer_auth_query: false # query postgres to retrieve unlisted business users?
    pgbouncer_poolmode: transaction # pooling mode: transaction,session,statement, transaction by default
    pgbouncer_sslmode: disable # pgbouncer client ssl mode, disable by default

    #-----------------------------------------------------------------
    # PG_PROVISION
    #-----------------------------------------------------------------
    pg_provision: true # provision postgres cluster after bootstrap
    pg_init: pg-init # provision init script for cluster template, `pg-init` by default
    pg_default_roles: # default roles and users in postgres cluster
      - {
          name: dbrole_readonly,
          login: false,
          comment: role for global read-only access,
        }
      - {
          name: dbrole_offline,
          login: false,
          comment: role for restricted read-only access,
        }
      - {
          name: dbrole_readwrite,
          login: false,
          roles: [dbrole_readonly],
          comment: role for global read-write access,
        }
      - {
          name: dbrole_admin,
          login: false,
          roles: [pg_monitor, dbrole_readwrite],
          comment: role for object creation,
        }
      - { name: postgres, superuser: true, comment: system superuser }
      - {
          name: replicator,
          replication: true,
          roles: [pg_monitor, dbrole_readonly],
          comment: system replicator,
        }
      - {
          name: dbuser_dba,
          superuser: true,
          roles: [dbrole_admin],
          pgbouncer: true,
          pool_mode: session,
          pool_connlimit: 16,
          comment: pgsql admin user,
        }
      - {
          name: dbuser_monitor,
          roles: [pg_monitor],
          pgbouncer: true,
          parameters: { log_min_duration_statement: 1000 },
          pool_mode: session,
          pool_connlimit: 8,
          comment: pgsql monitor user,
        }
    pg_default_privileges: # default privileges when created by admin user
      - GRANT USAGE      ON SCHEMAS   TO dbrole_readonly
      - GRANT SELECT     ON TABLES    TO dbrole_readonly
      - GRANT SELECT     ON SEQUENCES TO dbrole_readonly
      - GRANT EXECUTE    ON FUNCTIONS TO dbrole_readonly
      - GRANT USAGE      ON SCHEMAS   TO dbrole_offline
      - GRANT SELECT     ON TABLES    TO dbrole_offline
      - GRANT SELECT     ON SEQUENCES TO dbrole_offline
      - GRANT EXECUTE    ON FUNCTIONS TO dbrole_offline
      - GRANT INSERT     ON TABLES    TO dbrole_readwrite
      - GRANT UPDATE     ON TABLES    TO dbrole_readwrite
      - GRANT DELETE     ON TABLES    TO dbrole_readwrite
      - GRANT USAGE      ON SEQUENCES TO dbrole_readwrite
      - GRANT UPDATE     ON SEQUENCES TO dbrole_readwrite
      - GRANT TRUNCATE   ON TABLES    TO dbrole_admin
      - GRANT REFERENCES ON TABLES    TO dbrole_admin
      - GRANT TRIGGER    ON TABLES    TO dbrole_admin
      - GRANT CREATE     ON SCHEMAS   TO dbrole_admin
    pg_default_schemas: [monitor] # default schemas to be created
    pg_default_extensions: # default extensions to be created
      - { name: adminpack, schema: pg_catalog }
      - { name: pg_stat_statements, schema: monitor }
      - { name: pgstattuple, schema: monitor }
      - { name: pg_buffercache, schema: monitor }
      - { name: pageinspect, schema: monitor }
      - { name: pg_prewarm, schema: monitor }
      - { name: pg_visibility, schema: monitor }
      - { name: pg_freespacemap, schema: monitor }
      - { name: postgres_fdw, schema: public }
      - { name: file_fdw, schema: public }
      - { name: btree_gist, schema: public }
      - { name: btree_gin, schema: public }
      - { name: pg_trgm, schema: public }
      - { name: intagg, schema: public }
      - { name: intarray, schema: public }
      - { name: pg_repack }
    pg_reload: true # reload postgres after hba changes
    pg_default_hba_rules: # postgres default host-based authentication rules
      - {
          user: "${dbsu}",
          db: all,
          addr: local,
          auth: ident,
          title: "dbsu access via local os user ident",
        }
      - {
          user: "${dbsu}",
          db: replication,
          addr: local,
          auth: ident,
          title: "dbsu replication from local os ident",
        }
      - {
          user: "${repl}",
          db: replication,
          addr: localhost,
          auth: pwd,
          title: "replicator replication from localhost",
        }
      - {
          user: "${repl}",
          db: replication,
          addr: intra,
          auth: pwd,
          title: "replicator replication from intranet",
        }
      - {
          user: "${repl}",
          db: postgres,
          addr: intra,
          auth: pwd,
          title: "replicator postgres db from intranet",
        }
      - {
          user: "${monitor}",
          db: all,
          addr: localhost,
          auth: pwd,
          title: "monitor from localhost with password",
        }
      - {
          user: "${monitor}",
          db: all,
          addr: infra,
          auth: pwd,
          title: "monitor from infra host with password",
        }
      - {
          user: "${admin}",
          db: all,
          addr: infra,
          auth: ssl,
          title: "admin @ infra nodes with pwd & ssl",
        }
      - {
          user: "${admin}",
          db: all,
          addr: world,
          auth: ssl,
          title: "admin @ everywhere with ssl & pwd",
        }
      - {
          user: "+dbrole_readonly",
          db: all,
          addr: localhost,
          auth: pwd,
          title: "pgbouncer read/write via local socket",
        }
      - {
          user: "+dbrole_readonly",
          db: all,
          addr: intra,
          auth: pwd,
          title: "read/write biz user via password",
        }
      - {
          user: "+dbrole_offline",
          db: all,
          addr: intra,
          auth: pwd,
          title: "allow etl offline tasks from intranet",
        }
    pgb_default_hba_rules: # pgbouncer default host-based authentication rules
      - {
          user: "${dbsu}",
          db: pgbouncer,
          addr: local,
          auth: peer,
          title: "dbsu local admin access with os ident",
        }
      - {
          user: "all",
          db: all,
          addr: localhost,
          auth: pwd,
          title: "allow all user local access with pwd",
        }
      - {
          user: "${monitor}",
          db: pgbouncer,
          addr: intra,
          auth: pwd,
          title: "monitor access via intranet with pwd",
        }
      - {
          user: "${monitor}",
          db: all,
          addr: world,
          auth: deny,
          title: "reject all other monitor access addr",
        }
      - {
          user: "${admin}",
          db: all,
          addr: intra,
          auth: pwd,
          title: "admin access via intranet with pwd",
        }
      - {
          user: "${admin}",
          db: all,
          addr: world,
          auth: deny,
          title: "reject all other admin access addr",
        }
      - {
          user: "all",
          db: all,
          addr: intra,
          auth: pwd,
          title: "allow all user intra access with pwd",
        }

    #-----------------------------------------------------------------
    # PG_BACKUP
    #-----------------------------------------------------------------
    pgbackrest_enabled: true # enable pgbackrest on pgsql host?
    pgbackrest_clean: true # remove pg backup data during init?
    pgbackrest_log_dir: /pg/log/pgbackrest # pgbackrest log dir, `/pg/log/pgbackrest` by default
    pgbackrest_method: local # pgbackrest repo method: local,minio,[user-defined...]
    pgbackrest_repo: # pgbackrest repo: https://pgbackrest.org/configuration.html#section-repository
      local: # default pgbackrest repo with local posix fs
        path: /pg/backup # local backup directory, `/pg/backup` by default
        retention_full_type: count # retention full backups by count
        retention_full: 2 # keep 2, at most 3 full backup when using local fs repo
      minio: # optional minio repo for pgbackrest
        type: s3 # minio is s3-compatible, so s3 is used
        s3_endpoint: sss.pigsty # minio endpoint domain name, `sss.pigsty` by default
        s3_region: us-east-1 # minio region, us-east-1 by default, useless for minio
        s3_bucket: pgsql # minio bucket name, `pgsql` by default
        s3_key: pgbackrest # minio user access key for pgbackrest
        s3_key_secret: S3User.Backup # minio user secret key for pgbackrest
        s3_uri_style: path # use path style uri for minio rather than host style
        path: /pgbackrest # minio backup path, default is `/pgbackrest`
        storage_port: 9000 # minio port, 9000 by default
        storage_ca_file: /etc/pki/ca.crt # minio ca file path, `/etc/pki/ca.crt` by default
        bundle: y # bundle small files into a single file
        cipher_type: aes-256-cbc # enable AES encryption for remote backup repo
        cipher_pass: pgBackRest # AES encryption password, default is 'pgBackRest'
        retention_full_type: time # retention full backup by time on minio repo
        retention_full: 14 # keep full backup for last 14 days

    #-----------------------------------------------------------------
    # PG_SERVICE
    #-----------------------------------------------------------------
    pg_weight: 100 # relative load balance weight in service, 100 by default, 0-255
    pg_service_provider: "" # dedicate haproxy node group name, or empty string for local nodes by default
    pg_default_service_dest: pgbouncer # default service destination if svc.dest='default'
    pg_default_services: # postgres default service definitions
      - {
          name: primary,
          port: 5433,
          dest: default,
          check: /primary,
          selector: "[]",
        }
      - {
          name: replica,
          port: 5434,
          dest: default,
          check: /read-only,
          selector: "[]",
          backup: "[? pg_role == `primary` || pg_role == `offline` ]",
        }
      - {
          name: default,
          port: 5436,
          dest: postgres,
          check: /primary,
          selector: "[]",
        }
      - {
          name: offline,
          port: 5438,
          dest: postgres,
          check: /replica,
          selector: "[? pg_role == `offline` || pg_offline_query ]",
          backup: "[? pg_role == `replica` && !pg_offline_query]",
        }
    pg_vip_enabled: false # enable a l2 vip for pgsql primary? false by default
    pg_vip_address: 127.0.0.1/24 # vip address in `<ipv4>/<mask>` format, require if vip is enabled
    pg_vip_interface: eth0 # vip network interface to listen, eth0 by default
    pg_dns_suffix: "" # pgsql dns suffix, '' by default
    pg_dns_target: auto # auto, primary, vip, none, or ad hoc ip
